##################################
# Cromwell Reference Config File #
##################################

# This is the reference config file that contains all the default settings.
# The application config contains the default _override_ settings for cromwell.
# Make your edits/overrides in your cromwell.conf and be sure to `include required(classpath("application"))`.
# Add/See documented examples in cromwell.examples.conf.

webservice {
  port = 8000
  interface = 0.0.0.0
  binding-timeout = 5s
  instance.name = "reference"
}

akka {
  actor.default-dispatcher.fork-join-executor {
    # Number of threads = min(parallelism-factor * cpus, parallelism-max)
    # Below are the default values set by Akka, uncomment to tune these

    #parallelism-factor = 3.0
    #parallelism-max = 64
  }

  priority-mailbox {
    mailbox-type = "akka.dispatch.UnboundedControlAwareMailbox"
  }

  dispatchers {
    # A dispatcher for actors performing blocking io operations
    # Prevents the whole system from being slowed down when waiting for responses from external resources for instance
    io-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      # Using the forkjoin defaults, this can be tuned if we wish
    }

    # A dispatcher for actors handling API operations
    # Keeps the API responsive regardless of the load of workflows being run
    api-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher for engine actors
    # Because backends behavior is unpredictable (potentially blocking, slow) the engine runs
    # on its own dispatcher to prevent backends from affecting its performance.
    engine-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher used by supported backend actors
    backend-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher used for the service registry
    service-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher to bulkhead the health monitor from the rest of the system. Sets throughput low in order to
    # ensure the monitor is fairly low priority
    health-monitor-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 4
      }

      throughput = 1
    }
    # Note that without further configuration, all other actors run on the default dispatcher
  }

  coordinated-shutdown.phases {
    abort-all-workflows {
      # This phase is used to give time to Cromwell to abort all workflows upon shutdown.
      # It's only used if system.abort-jobs-on-terminate = true
      # This timeout can be adjusted to give more or less time to Cromwell to abort workflows
      timeout = 1 hour
      depends-on = [service-unbind]
    }

    stop-io-activity{
      # Adjust this timeout according to the maximum amount of time Cromwell
      # should be allowed to spend flushing its database queues
      timeout = 30 minutes
      depends-on = [service-stop]
    }
  }
}

system {
  # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  # Defaults to false in server mode, and true in run mode.
  # abort-jobs-on-terminate = false

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  graceful-server-shutdown = true

  # If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  workflow-restart = true

  # Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 5000

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 50

  # Workflows will be grouped by the value of the specified field in their workflow options.
  #
  # Currently, hog-safety will limit concurrent jobs within a hog group:
  # For each hog-group and backend, only (concurrent-job-limit / hog-factor) jobs will be run at any given time.
  #
  # In the future, hog-groups may also apply to other finite resources
  #
  # You can re-use an existing workflow option to allow it to also indicate hog-group,
  # or add a new field created specifically for hog-safety.
  #
  hog-safety {
    # Set this field in the workflow-options file to assign a hog group:
    workflow-option = "hogGroup"

    # Setting to '1' means that a hog group can use the full resources of the Cromwell instance if it needs to:
    hog-factor = 1

    # Time to wait before repeating token log messages - including "queue state" and "group X is a hog" style messages.
    # Leave at 0 to never log these types of message.
    token-log-interval-seconds = 0
  }

  # Number of seconds between workflow launches
  new-workflow-poll-rate = 20

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  number-of-cache-read-workers = 25

  # Maximum scatter width per scatter node. Cromwell will fail the workflow if the scatter width goes beyond N
  max-scatter-width-per-scatter = 1000000

  # Total max. jobs that can be created per root workflow. If it goes beyond N, Cromwell will fail the workflow by:
  # - no longer creating new jobs
  # - let the jobs that have already been started finish, and then fail the workflow
  total-max-jobs-per-root-workflow = 1000000

  io {
    # Global Throttling - This is mostly useful for GCS and can be adjusted to match
    # the quota availble on the GCS API
    number-of-requests = 100000
    per = 100 seconds

    # Number of times an I/O operation should be attempted before giving up and failing it.
    number-of-attempts = 5
    
    # configures exponential backoff of io requests
    backpressure-backoff {
      # starting point
      min = 10 seconds
      # maximum waiting time between attempts
      max = 5 minutes
      # how much longer to wait between each attempt
      multiplier = 2
      # randomizes wait times to avoid large spikes. Must be between 0 and 1.
      randomization-factor = 0.9
    }
    
    # Amount of time after which an I/O operation will timeout if no response has been received.
    # Note that a timeout may result in a workflow failure so be careful not to set a timeout too low.
    # Unless you start experiencing timeouts under very heavy load there should be no reason to change the default values.
    timeout {
      default = 3 minutes
      # Copy can be a time consuming operation and its timeout can be set separately.
      copy = 1 hour
    }
    
    gcs {
      parallelism = 10
    }
    
    nio {
      parallellism = 10
    }
  }

  # Maximum number of input file bytes allowed in order to read each type. 
  # If exceeded a FileSizeTooBig exception will be thrown.
  input-read-limits {

    lines = 512000000

    bool = 7

    int = 19

    float = 50

    string = 512000000

    json = 512000000

    tsv = 512000000

    map = 512000000

    object = 512000000
  }

  # Rate at which Cromwell updates its instrumentation gauge metrics (e.g: Number of workflows running, queued, etc..)
  instrumentation-rate = 5 seconds
  
  job-rate-control {
    jobs = 50
    per = 1 second
  }

  workflow-heartbeats {
    heartbeat-interval: 2 minutes
    ttl: 10 minutes
    write-failure-shutdown-duration: 5 minutes
    write-batch-size: 10000
    write-threshold: 10000
  }

  job-shell: "/bin/bash"
}

workflow-options {
  # These workflow options will be encrypted when stored in the database
  encrypted-fields: []

  # AES-256 key to use to encrypt the values in `encrypted-fields`
  base64-encryption-key: "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="

  # Directory where to write per workflow logs
  workflow-log-dir: "cromwell-workflow-logs"

  # When true, per workflow logs will be deleted after copying
  workflow-log-temporary: true

  # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.
  # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:
  #workflow-failure-mode: "ContinueWhilePossible"
}

# Optional call-caching configuration.
call-caching {
  # Allows re-use of existing results for jobs you've already run
  # (default: false)
  enabled = false

  # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
  # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
  # (default: true)
  invalidate-bad-cache-results = true
}

google {

  application-name = "cromwell"

  # See other auths examples in cromwell.examples.conf
  auths = [
    {
      name = "application-default"
      scheme = "application_default"
    }
  ]
}
# Filesystems available in this Crowmell instance
# They can be enabled individually in the engine.filesystems stanza and in the config.filesystems stanza of backends
# There is a default built-in local filesytem that can also be referenced as "local" as well.
filesystems {
  drs {
    class = "cromwell.filesystems.drs.DrsPathBuilderFactory"
    # Use to share a unique global object across all instances of the factory
    global {
      # Class to instantiate and propagate to all factories. Takes a single typesafe config argument
      class = "cromwell.filesystems.drs.DrsFileSystemConfig"
      config {
        martha {
          url = "https://martha-url-here"
          request.json-template = """{"url": "${drsPath}"}"""
        }
        access-token-acceptable-ttl = 1 minute
      }
    }
  }
  gcs {
    class = "cromwell.filesystems.gcs.GcsPathBuilderFactory"
  }
  oss {
    class = "cromwell.filesystems.oss.OssPathBuilderFactory"
  }
  s3 {
    class = "cromwell.filesystems.s3.S3PathBuilderFactory"
  }
  http {
    class = "cromwell.filesystems.http.HttpPathBuilderFactory"
  }
  ftp {
    # When the global configuration is used, the factory constructor needs to have a third argument of the type of the global configuration
    class = "cromwell.filesystems.ftp.FtpPathBuilderFactory"
    # Use to share a unique global object across all instances of the factory
    global {
      # Class to instantiate and propagate to all factories. Takes a single typesafe config argument
      class = "cromwell.filesystems.ftp.CromwellFtpFileSystems"
      # Configuration to be passed
      config {
        # All instances of FTP filesystems are being cached and re-used across a single Cromwell instance.
        # This allows control over how many connections are being established to a host for a specific user at any given time.
        # Cached filesystems are removed from the cache (and their associated connections closed) when they haven't been used for the
        # time configured below. This value should be sufficiently high to cover for the duration of the longest expected I/O operation.
        cache-ttl = 1 day
        # How long to wait trying to obtain a connection from the pool before giving up. Don't specify for no timeout
        # obtain-connection-timeout = 1 hour
        # Maximum number of connections that will be established per user per host. This is across the entire Cromwell instance
        max-connection-per-server-per-user = 30
        # Time after which a connection will be closed if idle
        idle-connection-timeout = 1 hour
        # FTP connection port to use
        connection-port: 21
        # FTP connection mode
        connection-mode = "passive"
      }
    }
  }
}

docker {
  hash-lookup {
    // /!\ Attention /!\
    // If you disable this call caching will be disabled for jobs with floating docker tags !
    enabled = true
    // Set this to match your available quota against the Google Container Engine API
    // This setting is deprecated, prefer using the gcr throttle configuration as shown below
    gcr-api-queries-per-100-seconds = 1000
    // Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again
    cache-entry-ttl = "20 minutes"
    // Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache
    cache-size = 200
    // How should docker hashes be looked up. Possible values are "local" and "remote"
    // "local": Lookup hashes on the local docker daemon using the cli
    // "remote": Lookup hashes on docker hub and gcr
    method = "remote"
    // For docker repositories that support the version 2 of the manifest schema, Cromwell can retrieve the compressed size
    // of the images. This compressed size is smaller than the actual size needed on disk when the docker image is pulled.
    // This factor is used to multiply the compressed size and get an approximation of the size needed on disk so that the 
    // disk can be allocated appropriately.
    // See https://github.com/docker/hub-feedback/issues/331
    size-compression-factor = 6.0
    
    // Retry configuration for http requests - across all registries
    max-time-between-retries = 1 minute
    max-retries = 3
    
    // Supported registries (dockerhub, gcr, quay) can have additional configuration set separately
    gcr {
      // Example of how to configure throttling, available for all supported registries 
      // throttle {
      //   number-of-requests = 1000
      //   per = 100 seconds
      // }
      
      // How many threads to allocate for gcr related work
      num-threads = 10
    }
    dockerhub.num-threads = 10
    quay.num-threads = 10
    alibabacloudcr.num-threads = 10
  }
}

engine {
  # This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.
  # For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.
  # If you intend to be able to run workflows with this kind of declarations:
  # workflow {
  #    String str = read_string("gs://bucket/my-file.txt")
  # }
  # You will need to provide the engine with a gcs filesystem
  # Note that the default filesystem (local) is always available.
  filesystems {
    local {
      enabled: true
    }
    http {
      enabled: true
    }
  }
}

languages {
  default: WDL
  WDL {
    versions {
      default: "draft-2"
      "draft-2" {
        language-factory = "languages.wdl.draft2.WdlDraft2LanguageFactory"
        config {
          strict-validation: false
          enabled: true
        }
      }
      "1.0" {
        # WDL draft-3 was our in-progress name for what became WDL 1.0
        language-factory = "languages.wdl.draft3.WdlDraft3LanguageFactory"
        config {
          strict-validation: true
          enabled: true
        }
      }
      "biscayne" {
        # WDL biscayne is our in-progress name for what will (probably) become WDL 1.1
        language-factory = "languages.wdl.biscayne.WdlBiscayneLanguageFactory"
        config {
          strict-validation: true
          enabled: true
        }
      }
    }
  }
  CWL {
    versions {
      default: "v1.0"
      "v1.0" {
        language-factory = "languages.cwl.CwlV1_0LanguageFactory"
        config {
          strict-validation: false
          enabled: true

          # Optional section to define a command returning paths to output files that
          # can only be known after the user's command has been run
          output-runtime-extractor {
            # Optional docker image on which the command should be run - Must have bash if provided
            docker-image = "stedolan/jq@sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e09"
            # Single line command executed after the tool has run.
            # It should write to stdout the file paths that are referenced in the cwl.output.json (or any other file path
            # not already defined in the outputs of the tool and therefore unknown when the tool is run)
            command = "cat cwl.output.json 2>/dev/null | jq -r '.. | .path? // .location? // empty'" 
          }
        }
      }
    }
  }
}

ontology {
  # Uncomment to enable caching of ontologies. Improves performance when loading ontologies from remote IRIs.
  #cache {
  #  max-size = 20
  #}
  retries = 3
  pool-size = 3
  backoff-time = 2 seconds
}


# Other backend examples are in cromwell.examples.conf
backend {
  default = "Local"
  providers {
    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        include required(classpath("reference_local_provider_config.inc.conf"))
      }
    }

  }
}

services {
  KeyValue {
    class = "cromwell.services.keyvalue.impl.SqlKeyValueServiceActor"
    config {
      # Similar to metadata service config, see cromwell.examples.conf
      # db-batch-size = 200
      # db-flush-rate = 5 seconds
    }
  }
  MetadataService {
    class = "cromwell.services.metadata.impl.MetadataServiceActor"
    config {
      # See cromwell.examples.conf for details on settings one can use here as they depend on the implementation
      # being used.
    }
  }
  Instrumentation {
    # Default noop service - instrumentation metrics are ignored
    class = "cromwell.services.instrumentation.impl.noop.NoopInstrumentationServiceActor"

    # StatsD instrumentation service actor
    # class = "cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActor"

    # Stackdriver instrumentation service actor
    # class = "cromwell.services.instrumentation.impl.stackdriver.StackdriverInstrumentationServiceActor"
  }
  HealthMonitor {
    class = "cromwell.services.healthmonitor.impl.HealthMonitorServiceActor"
    # Override the standard dispatcher. In particular this one has a low throughput so as to be lower in priority
    dispatcher = "akka.dispatchers.health-monitor-dispatcher"
    config {
      check-dockerhub: false
      check-engine-database: false
      check-gcs: false
      check-papi-backends: []
    }
  }
  LoadController {
    class = "cromwell.services.loadcontroller.impl.LoadControllerServiceActor"
    config {
      # See cromwell.examples.conf for details on settings one can use here
    }
  }
  Womtool {
    # Default - run within the Cromwell JVM
    class = "cromwell.services.womtool.impl.WomtoolServiceInCromwellActor"
  }
}

include required(classpath("reference_database.inc.conf"))

# Configuration for load-control related values 
load-control {
  ## Queue Size Thresholds ##
  # Cromwell centralizes some operations through singleton actors (possibly acting as routers).
  # This allows for a more efficient control, throttling, and potentially batching of those operations which overall improves throughput.
  # In order to do that, those operations are queued until they can be performed.
  # Those queues are for the most part unbounded in size, which can become a problem under heavy load.
  # Each actor can however let the load controller service know when it considers its work load to be abnormally high.
  # In the case of those queuing actors, this means that their queue size is over a certain threshold.
  # This section allows to configure those threshold values.
  # They should be kept at a reasonable number where reasonable will depend on your system and how much load Cromwell is submitted to.
  # If they're too high they could end up using a lot of memory, if they're too small any small spike will be considered a high load and the system will automatically slow itself down.
  # Benchmarking is recommended to find the values that suit your use case best.
  # If you use the statsD instrumentation service, the queue size and throughput of these actors are instrumented and looking at their value over time can also help you find the right configuration.
  
  job-store-read = 10000
  job-store-write = 10000
  # call cache read actors are routed (several actors are performing cache read operations
  # this threshold applies to each routee individually, so set it to a lower value to account for that
  # to change the number of routees, see the services.number-of-cache-read-workers config value
  call-cache-read = 1000
  call-cache-write = 10000
  key-value-read = 10000
  key-value-write = 10000
  # The I/O queue has the specificity to be bounded. This sets its size
  io-queue-size = 10000
  # If the I/O queue is full, subsequent requests are rejected and to be retried later.
  # This time window specifies how much time without request rejection consititutes a "back to normal load" event
  io-normal-window = 10000
  # metadata is an order of magnitude higher because its normal load is much higher than other actors
  metadata-write = 100000
  
  ## Backend specific ##
  # Google requests to the Pipelines API are also queued and batched
  papi-requests = 10000
  
  ## Misc. ##
  # How often each actor should update its perceived load
  monitoring-frequency = 5 seconds
}

ga4gh {
  wes {
    # URLs for site-specific auth process as well as contact info for security issues.
    # These aren't really compelling defaults, but there aren't really any compelling defaults
    auth-instructions-url = "https://cromwell.readthedocs.io/en/stable/"
    contact-info-url = "https://cromwell.readthedocs.io/en/stable/"
  }
}
