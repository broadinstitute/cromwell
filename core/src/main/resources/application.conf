webservice {
  port = 8000
  interface = 0.0.0.0
  instance.name = "reference"
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  actor {
    default-dispatcher {
      fork-join-executor {
        # Number of threads = min(parallelism-factor * cpus, parallelism-max)
        # Below are the default values set by Akka, uncomment to tune these

        #parallelism-factor = 3.0
        #parallelism-max = 64
      }
    }
  }

  dispatchers {
    # A dispatcher for actors performing blocking io operations
    # Prevents the whole system from being slowed down when waiting for responses from external resources for instance
    io-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      # Using the forkjoin defaults, this can be tuned if we wish
    }

    # A dispatcher for actors handling API operations
    # Keeps the API responsive regardless of the load of workflows being run
    api-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher for engine actors
    # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs
    # on its own dispatcher to prevent backends from affecting its performance.
    engine-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher used by supported backend actors
    backend-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # Note that without further configuration, all other actors run on the default dispatcher
  }
}

spray.can {
  server {
    request-timeout = 40s
  }
  client {
    request-timeout = 40s
    connecting-timeout = 40s
  }
}

system {
  // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  abort-jobs-on-terminate = false

  // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend
  max-retries = 10

  // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  workflow-restart = true

  // Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 5000

  // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 50

  // Number of seconds between workflow launches
  new-workflow-poll-rate = 20

  // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  number-of-workflow-log-copy-workers = 10
}

workflow-options {
  // These workflow options will be encrypted when stored in the database
  encrypted-fields: []

  // AES-256 key to use to encrypt the values in `encrypted-fields`
  base64-encryption-key: "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="

  // Directory where to write per workflow logs
  workflow-log-dir: "cromwell-workflow-logs"

  // When true, per workflow logs will be deleted after copying
  workflow-log-temporary: true

  // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.
  // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:
  //workflow-failure-mode: "ContinueWhilePossible"
}

// Optional call-caching configuration.
call-caching {
  enabled = false
}

google {

  application-name = "cromwell"

  auths = [
    {
      name = "application-default"
      scheme = "application_default"
    },
    {
      name = "user-via-refresh"
      scheme = "refresh_token"
      client-id = "secret_id"
      client-secret = "secret_secret"
    },
    {
      name = "service-account"
      scheme = "service_account"
      service-account-id = "my-service-account"
      pem-file = "/path/to/file.pem"
    }
  ]
}

engine {
  // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.
  // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.
  // If you intend to be able to run workflows with this kind of declarations:
  // workflow {
  //    String str = read_string("gs://bucket/my-file.txt")
  // }
  // You will need to provide the engine with a gcs filesystem
  // Note that the default filesystem (local) is always available.
  //filesystems {
  //  gcs {
  //    auth = "application-default"
  //  }
  //}
}

backend {
  default = "Local"
  providers {
    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        run-in-background = true
        runtime-attributes = "String? docker"
        submit = "/bin/bash ${script}"
        submit-docker = "docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"

        // Root directory where Cromwell writes job results.  This directory must be
        // visible and writeable by the Cromwell process as well as the jobs that Cromwell
        // launches.
        root: "cromwell-executions"

        filesystems {
          local {
            // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs
            // The following are strategies used to make those links.  They are ordered.  If one fails
            // The next one is tried:
            //
            // hard-link: attempt to create a hard-link to the file
            // copy: copy the file
            // soft-link: create a symbolic link to the file
            //
            // NOTE: soft-link will be skipped for Docker jobs
            localization: [
              "hard-link", "soft-link", "copy"
            ]
          }
        }
      }
    }

    //SGE {
    //  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    //  config {
    //    runtime-attributes = """
    //    Int cpu = 1
    //    Float? memory_gb
    //    String? sge_queue
    //    String? sge_project
    //    """
    //
    //    submit = """
    //    qsub \
    //    -terse \
    //    -V \
    //    -b n \
    //    -N ${job_name} \
    //    -wd ${cwd} \
    //    -o ${out} \
    //    -e ${err} \
    //    -pe smp ${cpu} \
    //    ${"-l m_mem_free=" + memory_gb + "gb"} \
    //    ${"-q " + sge_queue} \
    //    ${"-P " + sge_project} \
    //    ${script}
    //    """
    //
    //    kill = "qdel ${job_id}"
    //    check-alive = "qstat -j ${job_id}"
    //    job-id-regex = "(\\d+)"
    //  }
    //}

    //LSF {
    //  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    //  config {
    //    submit = "bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /bin/bash ${script}"
    //    kill = "bkill ${job_id}"
    //    check-alive = "bjobs ${job_id}"
    //    job-id-regex = "Job <(\\d+)>.*"
    //  }
    //}

    // Example backend that _only_ runs workflows that specify docker for every command.
    //Docker {
    //  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    //  config {
    //    run-in-background = true
    //    runtime-attributes = "String docker"
    //    submit-docker = "docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"
    //  }
    //}

    //    HtCondor {
    //      actor-factory = "cromwell.backend.impl.htcondor.HtCondorBackendFactory"
    //      config {
    //        // Root directory where Cromwell writes job results.  This directory must be
    //        // visible and writeable by the Cromwell process as well as the jobs that Cromwell
    //        // launches.
    //        root: "cromwell-executions"
    //
    //        cache {
    //          provider = "cromwell.backend.impl.htcondor.caching.provider.mongodb.MongoCacheActorFactory"
    //          enabled = true
    //          forceRewrite = false
    //          db {
    //            host = "127.0.0.1"
    //            port = 27017
    //            name = "htcondor"
    //            collection = "cache"
    //          }
    //        }
    //
    //        filesystems {
    //          local {
    //            // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs
    //            // The following are strategies used to make those links.  They are ordered.  If one fails
    //            // The next one is tried:
    //            //
    //            // hard-link: attempt to create a hard-link to the file
    //            // copy: copy the file
    //            // soft-link: create a symbolic link to the file
    //            //
    //            // NOTE: soft-link will be skipped for Docker jobs
    //            localization: [
    //              "hard-link", "soft-link", "copy"
    //            ]
    //          }
    //        }
    //      }
    //    }

    //Spark {
    // actor-factory = "cromwell.backend.impl.spark.SparkBackendFactory"
    // config {
    //   // Root directory where Cromwell writes job results.  This directory must be
    //    // visible and writeable by the Cromwell process as well as the jobs that Cromwell
    //   // launches.
    //   root: "cromwell-executions"

    //   filesystems {
    //     local {
    //        // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs
    //        // The following are strategies used to make those links.  They are ordered.  If one fails
    //       // The next one is tried:
    //       //
    //       // hard-link: attempt to create a hard-link to the file
    //       // copy: copy the file
    //        // soft-link: create a symbolic link to the file
    //       //
    //       // NOTE: soft-link will be skipped for Docker jobs
    //       localization: [
    //         "hard-link", "soft-link", "copy"
    //       ]
    //     }
    //    }
    //      // change (master, deployMode) to (yarn, client), (yarn, cluster)
    //      //  or (spark://hostname:port, cluster) for spark standalone cluster mode
    //     master: "local"
    //     deployMode: "client"
    //  }
    // }

    //JES {
    //  actor-factory = "cromwell.backend.impl.jes.JesBackendLifecycleActorFactory"
    //  config {
    //    // Google project
    //    project = "my-cromwell-workflows"
    //
    //    // Base bucket for workflow executions
    //    root = "gs://my-cromwell-workflows-bucket"
    //
    //    // Polling for completion backs-off gradually for slower-running jobs.
    //    // This is the maximum polling interval (in seconds):
    //    maximum-polling-interval = 600
    //
    //    // Optional Dockerhub Credentials. Can be used to access private docker images.
    //    dockerhub {
    //      // account = ""
    //      // token = ""
    //    }
    //
    //    genomics {
    //      // A reference to an auth defined in the `google` stanza at the top.  This auth is used to create
    //      // Pipelines and manipulate auth JSONs.
    //      auth = "application-default"
    //      // Endpoint for APIs, no reason to change this unless directed by Google.
    //      endpoint-url = "https://genomics.googleapis.com/"
    //    }
    //
    //    filesystems {
    //      gcs {
    //        // A reference to a potentially different auth for manipulating files via engine functions.
    //        auth = "application-default"
    //      }
    //    }
    //  }
    //}

  }
}

services {
  KeyValue {
    class = "cromwell.services.keyvalue.impl.SqlKeyValueServiceActor"
  }
  MetadataService {
    class = "cromwell.services.metadata.impl.MetadataServiceActor"
  }
}

database {
  // This specifies which database to use
  config = main.hsqldb

  main {
    hsqldb {
      driver = "slick.driver.HsqldbDriver$"
      db {
        driver = "org.hsqldb.jdbcDriver"
        url = "jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc"
        connectionTimeout = 3000
      }
    }

    mysql_example {
      driver = "slick.driver.MySQLDriver$"
      db {
        driver = "com.mysql.jdbc.Driver"
        url = "jdbc:mysql://host/cromwell"
        user = "user"
        password = "pass"
        connectionTimeout = 5000
      }

      migration {
        read-batch-size = 100000
        write-batch-size = 100000
      }
    }
  }
}
