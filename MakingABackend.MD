# Making a backend

## Part 0: Introduction

- These notes were added while making a new AWS backend for Amazon AWS.

## Part 1 (October 13 2016): The skeleton:

To start with, I just need to create a bunch of boilerplate which will eventually be filled in with all of the lovely AWS details!

### Defining the awsBackend project:

- Added entries to `project/Settings.scala`, `project/Dependencies.scala` and `build.sbt`
- This was mainly just a copy/paste from existing backend projects. I made a few typos renaming everything and linking the dependencies properly though!
- E.g. In my first commit I forgot to update the libraryDependencies name for my AWS backend project:
```
  val awsBackendSettings = List(
    name := "cromwell-aws-backend",
    libraryDependencies ++= awsBackendDependencies
  ) ++ commonSettings
```
- I guessed that I'd need the AWS SDK so I included that immediately in Dependencies.scala:
```
  val awsBackendDependencies = List(
    "com.amazonaws" % "aws-java-sdk" % "1.11.41"
  )
```
- In build.scala I had to also edit the `lazy val root` to include a new `.aggregate(awsBackend)` and a new `.dependsOn(awsBackend)`

### Directory structure:

- This is probably going to be autogenerated for you in the directories specified in the above files. I'd already added my own directory structure and sbt managed to pick it up correctly in `supportedBackends/aws`.

### AWS Job Execution Actor:
- To run a job, Cromwell needs to instantiate a Job Execution actor. I'll fill in the details later but for now, I'll just add the constructor, props, and an unimplemented method definition for `execute`:
```
class AwsJobExecutionActor(override val jobDescriptor: BackendJobDescriptor,
                           override val configurationDescriptor: BackendConfigurationDescriptor) extends BackendJobExecutionActor {

  override def execute: Future[BackendJobExecutionResponse] = ???
}

object AwsJobExecutionActor {
  def props(jobDescriptor: BackendJobDescriptor,
            configurationDescriptor: BackendConfigurationDescriptor): Props = Props(new AwsJobExecutionActor(jobDescriptor, configurationDescriptor))
}
```

### Actor factory:
- This is the class which tells Cromwell which classes represent job execution actors, initialization actors and so on. I'm just adding a skeleton for now, with a constructor of the form the Cromwell expects:
```
case class AwsBackendActorFactory(name: String, configurationDescriptor: BackendConfigurationDescriptor) extends BackendLifecycleActorFactory {

  override def jobExecutionActorProps(jobDescriptor: BackendJobDescriptor,
                                      initializationData: Option[BackendInitializationData],
                                      serviceRegistryActor: ActorRef,
                                      backendSingletonActor: Option[ActorRef]): Props = AwsJobExecutionActor.props(jobDescriptor, configurationDescriptor)
}
```
- There are a few other actor definitions that can be added to this file over time. But the only one that Cromwell *requires* to work is the job execution actor.

### Reference conf:

- Reference.conf is a set of reference options which shows people how to enable the backends that they want. So I'll add the initial config which people would add if they wanted the AWS backend (commented out in the reference so it's not enabled by default). This goes below all the other backend references:
```
    #AWS {
    #  actor-factory = "cromwell.backend.impl.aws.AwsBackendActorFactory"
    #  config {
    #
    #  }
    #}
```

### Application.conf

- OK so I've now told people how to add this backend... Now I actually add it to my own personal configuration file so I can try it out!
```
backend {
  default = "AWS"
  providers {
    AWS {
      actor-factory = "cromwell.backend.impl.aws.AwsBackendActorFactory"
      config {
      
      }
    }
  }
}
```

### Trying it out
So we now have a backend skeleton! What happens when we run it? Well hopefully Cromwell will instantiate the backend far enough to reach the unimplemented execute method and then fall over. Let's give it a go!
- I fire up cromwell in server mode with my modified application.conf.
- I create a sample WDL that would sleep for 20 seconds if it actually worked:
The input WDL:
```
task sleep {
  command { sleep 20 }
}
workflow main {
  call sleep
}
```
- I submit the WDL to the swagger endpoint (http://localhost:8000/swagger/index.html?url=/swagger/cromwell.yaml) and watch the server logs...
- And as expected:
```
2016-10-13 13:14:29,017 cromwell-system-akka.dispatchers.engine-dispatcher-39 INFO  - MaterializeWorkflowDescriptorActor [UUID(ddd827ba)]: Call-to-Backend assignments: main.sleep -> AWS
2016-10-13 13:14:30,167 cromwell-system-akka.dispatchers.engine-dispatcher-39 INFO  - WorkflowExecutionActor-ddd827ba-091f-4c6f-b98f-cc9825717007 [UUID(ddd827ba)]: Starting calls: main.sleep:NA:1
2016-10-13 13:14:30,983 cromwell-system-akka.actor.default-dispatcher-5 ERROR - guardian failed, shutting down system
scala.NotImplementedError: an implementation is missing
  at scala.Predef$.$qmark$qmark$qmark(Predef.scala:230)
  at cromwell.backend.impl.aws.AwsJobExecutionActor.execute(AwsJobExecutionActor.scala:12)
```
- OK, so now I just need to implement `execute(): Future[JobExecutionResult]` and Cromwell can interface with AWS. How hard can it be!

## Part 2 (October 13 2016): Using Amazon to sleep 20 seconds

### Starting point
- This was a learning experience after using the Google pipelines service to submit jobs! 
- To get myself started, I've manually created an ECS cluster which I've called `ecs-t2micro-cluster` via the ECS web console.

### Trial and Error

- I see in the aws sdk docs that there's an AmazonECSAsyncClient class. That sounds promising! Luckily I already added the dependency on AWS SDK in Part 1 so I guess I can just write something basic in my AwsJobExecutionActor class and see what happens:

- I ended up having to add some credentials options to the configuration file. The new `reference.conf` now looks like:
```
    #AWS {
    #  actor-factory = "cromwell.backend.impl.aws.AwsBackendActorFactory"
    #  config {
    #    ## These two settings are required to authenticate with the ECS service:
    #    accessKeyId = "..."
    #    secretKey = "..."
    #  }
    #}
``` 

- After a little bit of experimentation with the ECS API, I was able to come up with a backend that works but is very limited... It is entirely synchronous in the `execute` method. That's certainly not a final answer but it works OK for running a single task. And we can now run that single `sleep` command successfully on the Amazon EC2 Container Service!
  - The synchronous `execute` method:
```
class AwsJobExecutionActor(override val jobDescriptor: BackendJobDescriptor,
                           override val configurationDescriptor: BackendConfigurationDescriptor) extends BackendJobExecutionActor {

  val awsAccessKeyId = configurationDescriptor.backendConfig.as[String]("accessKeyId")
  val awsSecretKey = configurationDescriptor.backendConfig.as[String]("secretKey")

  val clusterName = "ecs-t2micro-cluster"

  val credentials = new AWSCredentials {
    override def getAWSAccessKeyId: String = awsAccessKeyId
    override def getAWSSecretKey: String = awsSecretKey
  }
  val ecsAsyncClient = new AmazonECSAsyncClient(credentials)

  override def execute: Future[BackendJobExecutionResponse] = {

    val commandOverride = new ContainerOverride().withName("simple-app").withCommand(jobDescriptor.call.instantiateCommandLine(Map.empty, OnlyPureFunctions, identity).get)

    val runRequest: RunTaskRequest = new RunTaskRequest()
      .withCluster(clusterName)
      .withCount(1)
      .withTaskDefinition("ubuntuTask:1")
      .withOverrides(new TaskOverride().withContainerOverrides(commandOverride))

    val submitResultHandler = new AwsSdkAsyncHandler[RunTaskRequest, RunTaskResult]()
    val _ = ecsAsyncClient.runTaskAsync(runRequest, submitResultHandler)

    submitResultHandler.future map {
      case AwsSdkAsyncResult(_, result) =>
        log.info("AWS submission completed:\n{}", result.toString)
        val taskArn= result.getTasks.asScala.head.getTaskArn
        val taskDescription = waitUntilDone(taskArn)

        log.info("AWS task completed!\n{}", taskDescription.toString)
        SucceededResponse(jobDescriptor.key, Option(0), Map.empty, None, Seq.empty)
    }
  }

  private def waitUntilDone(taskArn: String): Task = {
    val describeTasksRequest = new DescribeTasksRequest().withCluster(clusterName).withTasks(List(taskArn).asJava)

    val resultHandler = new AwsSdkAsyncHandler[DescribeTasksRequest, DescribeTasksResult]()
    val _ = ecsAsyncClient.describeTasksAsync(describeTasksRequest, resultHandler)

    val desribedTasks = Await.result(resultHandler.future, Duration.Inf)
    val taskDescription = desribedTasks.result.getTasks.asScala.head
    if (taskDescription.getLastStatus == DesiredStatus.STOPPED.toString) {
      taskDescription
    } else {
      Thread.sleep(200)
      waitUntilDone(taskArn)
    }
  }
}
```

## Part 3 (October 26 2016): Moving synchronous polling to asynchronous polling

After getting a synchronous actor working, the next steps were to take the polling `waitUntilDone` and migrate it to an asynchronous version.

### Beginning AWS Async Job Execution Actor:

- We began by creating an empty `AsyncBackendJobExecutionActor` extending the prerequisite traits, and adding stubs for the functions we'll need to fill in.

```scala
package cromwell.backend.impl.aws

import akka.actor.{Actor, ActorLogging, ActorRef}
import cromwell.backend.BackendJobExecutionActor.BackendJobExecutionResponse
import cromwell.backend.{BackendConfigurationDescriptor, BackendJobDescriptor, BackendJobLifecycleActor}
import cromwell.backend.async.{AsyncBackendJobExecutionActor, ExecutionHandle}
import cromwell.backend.async.AsyncBackendJobExecutionActor.ExecutionMode

import scala.concurrent.{ExecutionContext, Promise}

class AwsAsyncJobExecutionActor(override val jobDescriptor: BackendJobDescriptor,
                                override val completionPromise: Promise[BackendJobExecutionResponse],
                                val configurationDescriptor: BackendConfigurationDescriptor,
                                val serviceRegistryActor: ActorRef)
  extends Actor with ActorLogging with BackendJobLifecycleActor with AsyncBackendJobExecutionActor {

  override def retryable = ???

  override def executeOrRecoverBackOff = ???

  override def executeOrRecover(mode: ExecutionMode)(implicit ec: ExecutionContext) = ???

  override def pollBackOff = ???

  override def poll(previous: ExecutionHandle)(implicit ec: ExecutionContext) = ???
}
```

### Tracking the job:

- The asynchronous system needs a couple of classes to keep track of the running job and its status.

- The job will be tracked with an extension of the `JobId` trait, and stored in the key value store under a custom key "aws_task_arn".

```scala
case class AwsJobId(taskArn: String) extends JobId

object AwsJobId {
  val JobIdKey = "aws_task_arn"
}
```

- The run status will be tracked by wrapping the task in a case class.

```scala
case class AwsRunStatus(task: Task)
```

### Copying in an async implementation:

- We began by implementing the stubs using (aka copying and pasting) similar code from other backends, specifically the SFS and Jes backends.

```scala
override def retryable = false

override def executeOrRecoverBackOff = SimpleExponentialBackoff(
  initialInterval = 3.seconds, maxInterval = 20.seconds, multiplier = 1.1)

override def executeOrRecover(mode: ExecutionMode)(implicit ec: ExecutionContext): Future[ExecutionHandle] = {
  Future.fromTry(Try {
    mode match {
      case Recover(jobId: BackendJobId) => recover(jobId)
      case _ => execute()
    }
  })
}

override lazy val pollBackOff = SimpleExponentialBackoff(
  initialInterval = 30.seconds, maxInterval = 600.seconds, multiplier = 1.1)

override def poll(previous: ExecutionHandle)(implicit ec: ExecutionContext): Future[ExecutionHandle] = {
  Future.fromTry(Try {
    previous match {
      case handle: PendingExecutionHandle[
        BackendJobId@unchecked, BackendRunInfo@unchecked, BackendRunStatus@unchecked] =>

        jobLogger.debug(s"$tag Polling Job ${handle.pendingJob}")
        Try(pollStatus(handle.pendingJob)) match {
          case Success(backendRunStatus) => updateExecutionHandleSuccess(handle, backendRunStatus)
          case Failure(throwable) => updateExecutionHandleFailure(handle, throwable)
        }
      case f: FailedNonRetryableExecutionHandle => f
      case s: SuccessfulExecutionHandle => s
      case badHandle => throw new IllegalArgumentException(s"Unexpected execution handle: $badHandle")
    }
  })
}

private def updateExecutionHandleSuccess(oldHandle: BackendPendingExecutionHandle,
                                         status: BackendRunStatus): ExecutionHandle = {
  val previousStatus = oldHandle.previousStatus
  if (!(previousStatus contains status)) {
    // If this is the first time checking the status, we log the transition as '-' to 'currentStatus'. Otherwise
    // just use the state names.
    val prevStateName = previousStatus.map(_.toString).getOrElse("-")
    jobLogger.info(s"$tag Status change from $prevStateName to $status")
    tellMetadata(Map("backendStatus" -> status))
  }

  status match {
    case _ if isTerminal(status) =>
      val metadata = getTerminalMetadata(status)
      tellMetadata(metadata)
      executionResult(status, oldHandle)
    case s => oldHandle.copy(previousStatus = Option(s)) // Copy the current handle with updated previous status.
  }
}

private def updateExecutionHandleFailure(oldHandle: BackendPendingExecutionHandle,
                                         throwable: Throwable): ExecutionHandle = {
  throwable match {
    case exception: Exception =>
      val handler: PartialFunction[Exception, ExecutionHandle] =
        customPollStatusFailure(oldHandle) orElse {
          case exception: Exception =>
            // Log exceptions and return the original handle to try again.
            jobLogger.warn(s"Caught exception, retrying", exception)
            oldHandle
        }
      handler(exception)
    case error: Error => throw error // JVM-ending calamity.
    case _: Throwable =>
      // Someone has subclassed or instantiated Throwable directly. Kill the job. They should be using an Exception.
      FailedNonRetryableExecutionHandle(throwable)
  }
}

private def tellMetadata(metadataKeyValues: Map[String, Any]): Unit = {
  import cromwell.services.metadata.MetadataService.implicits.MetadataAutoPutter
  serviceRegistryActor.putMetadata(jobDescriptor.workflowDescriptor.id, Option(jobDescriptor.key), metadataKeyValues)
}

private def executionResult(status: BackendRunStatus, handle: BackendPendingExecutionHandle)
                           (implicit ec: ExecutionContext): ExecutionHandle = {
  try {
    if (isSuccess(status)) {

      lazy val stderrLength: Long = File(remoteStrErrPath).size
      lazy val returnCode: Try[Int] = returnCodeContents.map(_.trim.toInt)
      status match {
        case _ if failOnStderr && stderrLength.intValue > 0 =>
          // returnCode will be None if it couldn't be downloaded/parsed, which will yield a null in the DB
          FailedNonRetryableExecutionHandle(new RuntimeException(
            s"execution failed: stderr has length $stderrLength"), returnCode.toOption)
        case _ if returnCodeContents.isFailure =>
          val exception = returnCode.failed.get
          jobLogger.warn(s"could not download return code file, retrying", exception)
          // Return handle to try again.
          handle
        case _ if returnCode.isFailure =>
          FailedNonRetryableExecutionHandle(new RuntimeException(
            s"execution failed: could not parse return code as integer: ${returnCodeContents.get}"))
        case _ if !continueOnReturnCode.continueFor(returnCode.get) =>
          val badReturnCodeMessage = s"Call ${jobDescriptor.key}: return code was ${returnCode.getOrElse("(none)")}"
          FailedNonRetryableExecutionHandle(new RuntimeException(badReturnCodeMessage), returnCode.toOption)
        case _ =>
          handleSuccess(status)
      }

    } else {
      handleFailure(status)
    }
  } catch {
    case e: Exception =>
      jobLogger.warn("Caught exception processing job result, retrying", e)
      // Return the original handle to try again.
      handle
  }
}

private lazy val instantiatedCommand = Command.instantiate(
  jobDescriptor, commandLineFunctions, commandLinePreProcessor, commandLineValueMapper).get

private lazy val tag = s"${this.getClass.getSimpleName} [UUID(${workflowId.shortString}):${jobDescriptor.key.tag}]"
```

### Adding our specific async implementation:

- There are a few types used above, mainly concerned with the pending execution handle, the job id, and the run status. We'll first start with filling in the type aliases:

```scala
type BackendJobId = AwsJobId
type BackendRunInfo = Any
type BackendRunStatus = AwsRunStatus
type BackendPendingExecutionHandle = PendingExecutionHandle[BackendJobId, BackendRunInfo, BackendRunStatus]
```

- Next we added basic implementations of the file accessors. These will be will be filled in during a future date.

```scala
private def commandLineFunctions: WdlFunctions[WdlValue] = OnlyPureFunctions

private def commandLinePreProcessor: EvaluatedTaskInputs => Try[EvaluatedTaskInputs] = Success.apply

private def commandLineValueMapper: WdlValue => WdlValue = identity

private def remoteStrErrPath: Path = ???

private lazy val returnCodeContents: Try[String] = Try("0")
```

- Then we added basic implementations of the standard runtime attributes. Again, these will be will be filled in at a future date.

```scala
private def continueOnReturnCode: ContinueOnReturnCode = ContinueOnReturnCodeFlag(false)

private def failOnStderr: Boolean = false
```

- We then migrate over our aws configuration code:
```scala
private val awsAccessKeyId: String = configurationDescriptor.backendConfig.as[String]("accessKeyId")
private val awsSecretKey: String = configurationDescriptor.backendConfig.as[String]("secretKey")
private val clusterName: String =
  configurationDescriptor.backendConfig.getOrElse("clusterName", "ecs-t2micro-cluster")

private val credentials: AWSCredentials = new BasicAWSCredentials(awsAccessKeyId, awsSecretKey)
private val ecsAsyncClient: AmazonECSAsync = new AmazonECSAsyncClient(credentials)
```

- With all of the above, we can now implement our `execute`. It will return a handle that may be used for asynchronous polling:

```scala
private def execute(): ExecutionHandle = {
  val commandOverride = new ContainerOverride()
    .withName("simple-app")
    .withCommand(commandLine)

  val taskOverride = new TaskOverride()
    .withContainerOverrides(commandOverride)

  val runTaskRequest = new RunTaskRequest()
    .withCluster(clusterName)
    .withCount(1)
    .withTaskDefinition("ubuntuTask:1")
    .withOverrides(taskOverride)

  val runTaskResult = ecsAsyncClient.runTask(runTaskRequest)

  log.info("AWS submission completed:\n{}", runTaskResult)
  val taskArn = runTaskResult.getTasks.asScala.head.getTaskArn

  PendingExecutionHandle(jobDescriptor, AwsJobId(taskArn), None, None)
}
```

- We also need a recover, but we'll implement that later. When asked to recover, for now we'll just start a new execution:

```scala
private def recover(jobId: BackendJobId): ExecutionHandle = execute()
```

- The polling status will check the job id and return a new status instance:

```scala
private def pollStatus(pendingJob: BackendJobId): BackendRunStatus = {
  val taskArn = pendingJob.taskArn

  val describeTasksRequest = new DescribeTasksRequest()
    .withCluster(clusterName)
    .withTasks(List(taskArn).asJava)

  val describeTasksResult = ecsAsyncClient.describeTasks(describeTasksRequest)

  val tasks = describeTasksResult.getTasks.asScala
  val task = tasks.head
  AwsRunStatus(task)
}
```

- For now, we won't do anything special for custom polling failures:

```scala
private def customPollStatusFailure(oldHandle: BackendPendingExecutionHandle):
PartialFunction[Exception, ExecutionHandle] = {
  PartialFunction.empty
}
```

- As in the synchronous version, we will stop polling when the aws status is STOPPED:

```scala
private def isTerminal(runStatus: BackendRunStatus): Boolean = {
  runStatus.task.getLastStatus == DesiredStatus.STOPPED.toString
}
```

- For now, we will ignore aws job status, and not return any custom job metadata:

```scala
private def getTerminalMetadata(runStatus: BackendRunStatus): Map[String, Any] = {
  // TODO: Get terminal metadata
  Map.empty
}

private def isSuccess(runStatus: BackendRunStatus): Boolean = {
  // TODO: Discriminate failure statuses
  true
}
```

- The last thing to do will be to handle job completion, with either a success or failure handle:

```scala
private def handleSuccess(runStatus: BackendRunStatus, handle: BackendPendingExecutionHandle,
                          returnCode: Int): ExecutionHandle = {
  // TODO: Processes success
  log.info("AWS task completed!\n{}", runStatus.task)
  SuccessfulExecutionHandle(Map.empty, returnCode, Map.empty, Seq.empty, None)
}

private def handleFailure(runStatus: BackendRunStatus, handle: BackendPendingExecutionHandle): ExecutionHandle = {
  // TODO: Process failure
  FailedNonRetryableExecutionHandle(new Exception(s"Task failed for unknown reason: ${runStatus.task}"), None)
}
```

### Refactoring sync actor to call the async actor

- With the async actor containing basic wiring, we can now update the synchronous actor to call into our asynchronous version and then wait. The synchronous actor constructor starts out with a similar signature. In this updated version, we'll also pass in the service registry actor, and specify the backend dispatcher when creating Props:

```scala
class AwsJobExecutionActor(override val jobDescriptor: BackendJobDescriptor,
                           override val configurationDescriptor: BackendConfigurationDescriptor,
                           serviceRegistryActor: ActorRef) extends BackendJobExecutionActor {

  override def execute: Future[BackendJobExecutionResponse] = ???
}

object AwsJobExecutionActor {
  def props(jobDescriptor: BackendJobDescriptor,
            configurationDescriptor: BackendConfigurationDescriptor,
            serviceRegistryActor: ActorRef): Props = {
    Props(
      new AwsJobExecutionActor(jobDescriptor, configurationDescriptor, serviceRegistryActor)
    ).withDispatcher(Dispatcher.BackendDispatcher)
  }
}
```

- From there, we'll now implement the execute method using (again copying and pasting) most of the synchronous backend from SFS and/or Jes:

```scala
context.become(startup orElse super.receive)

private def startup: Receive = {
  case AbortJobCommand =>
    context.parent ! AbortedResponse(jobDescriptor.key)
    context.stop(self)
}

private def running(executor: ActorRef): Receive = {
  case AbortJobCommand =>
    executor ! AbortJobCommand
  case abortResponse: AbortedResponse =>
    context.parent ! abortResponse
    context.stop(self)
  case KvPair(key, id@Some(jobId)) if key.key == jobIdKey =>
    // Successful operation ID lookup during recover.
    executor ! recoverMessage(jobId)
  case KvKeyLookupFailed(_) =>
    // Missed operation ID lookup during recover, fall back to execute.
    executor ! Execute
  case KvFailure(_, e) =>
    // Failed operation ID lookup during recover, crash and let the supervisor deal with it.
    completionPromise.tryFailure(e)
    throw new RuntimeException(s"Failure attempting to look up job id for key ${jobDescriptor.key}", e)
}

/**
  * This "synchronous" actor isn't finished until this promise finishes over in the asynchronous version.
  */
private lazy val completionPromise = Promise[BackendJobExecutionResponse]()

override def execute: Future[BackendJobExecutionResponse] = {
  val executorRef = context.actorOf(asyncProps, "SharedFileSystemAsyncJobExecutionActor")
  context.become(running(executorRef) orElse super.receive)
  executorRef ! Execute
  completionPromise.future
}

override def recover: Future[BackendJobExecutionResponse] = {
  val executorRef = context.actorOf(asyncProps, "SharedFileSystemAsyncJobExecutionActor")
  context.become(running(executorRef) orElse super.receive)
  val kvJobKey =
    KvJobKey(jobDescriptor.key.call.fullyQualifiedName, jobDescriptor.key.index, jobDescriptor.key.attempt)
  val kvGet = KvGet(ScopedKey(jobDescriptor.workflowDescriptor.id, kvJobKey, jobIdKey))
  serviceRegistryActor ! kvGet
  completionPromise.future
}

override def abort() = {
  throw new NotImplementedError("Abort is implemented via a custom receive of the message AbortJobCommand.")
}
```

- What's left is to add our custom aws implementation pointing to our job id, job id key, and aws async actor:

```scala
private val jobIdKey = AwsJobId.JobIdKey

private def recoverMessage(jobId: String): Recover = {
  Recover(AwsJobId(jobId))
}

private lazy val asyncProps: Props = {
  Props(
    new AwsAsyncJobExecutionActor(jobDescriptor, completionPromise, configurationDescriptor, serviceRegistryActor)
  ).withDispatcher(Dispatcher.BackendDispatcher)
}
```
