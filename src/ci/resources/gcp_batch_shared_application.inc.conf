include required(classpath("application.conf"))
include "build_application.inc.conf"
include "gcp_batch_application.inc.conf"

services {
  HealthMonitor.config {
    check-gcpbatch-backends: [
      "GCPBATCH",
    ]
  }
}

backend {
  default = "GCPBATCH"
  # pseudo GCPBATCH backends referenced in Centaur test files:
  # GCPBATCH_ALT: a GCPBATCH version of a PAPI v2 test exists, usually having the same name as the PAPI v2 test file with a "gcpbatch_" prefix.
  # GCPBATCH_NEEDS_ALT: a GCPBATCH alt of a PAPI v2 test is needed but does not yet exist.
  # GCPBATCH_FAIL: a test is failing on GCPBATCH, reasons may or may not be understood yet.
  enabled = ["GCPBATCH", "GCPBATCHRequesterPays", "GCPBATCH-Reference-Disk-Localization"]
  providers {
    # Default gcp batch backend
    GCPBATCH {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        # TODO: Should not need because already included.  testing.
        include "gcp_batch_provider_config.inc.conf"

        include "dockerhub_provider_config_v2.inc.conf"
        # This SA does not have permission to bill this project when accessing RP buckets.
        # This is on purpose so that we can assert the failure (see requester_pays_localization_negative)
        batch.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
      }
    }
    GCPBATCH-Reference-Disk-Localization {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        include "dockerhub_provider_config_v2.inc.conf"
        batch.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        
        filesystems.http {}

        include "google_reference_image_manifest.conf"
      }
    }  
    GCPBATCHRequesterPays {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        include "dockerhub_provider_config_v2.inc.conf"
        filesystems.gcs.auth = "requester_pays_service_account"
        # In order to function as the compute service account in GCP Batch, the service account specified here must have
        # Batch Agent Reporter role.  See WX-1576 and the tickets / docs linked from there for more info.
        batch.compute-service-account = "requester-pays-authorized@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
      }
    }
    GCPBATCH-usa {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        project = "user_error: google_project must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        root = "user_error: jes_gcs_root must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        batch.compute-service-account = "user_error: google_compute_service_account must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        batch.auth = "user_service_account"
        filesystems.http {}
        filesystems.drs.auth = "user_service_account"
        filesystems.gcs.auth = "user_service_account"
        filesystems.gcs.project = "user_error: user_service_account must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"

        # Have the engine authenticate to docker.io. See BT-141 for more info.
        include "dockerhub_provider_config_v1.inc.conf"
      }
    }
    GCPBATCHParallelCompositeUploads {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        include "dockerhub_provider_config_v2.inc.conf"
        # This SA does not have permission to bill this project when accessing RP buckets.
        # This is on purpose so that we can assert the failure (see requester_pays_localization_negative)
        batch.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        # Files larger than 150M should be delocalized using parallel composite uploading.
        batch.parallel-composite-upload-threshold = 150M
        filesystems.http {}
      }
    }
    GCPBATCH-Virtual-Private-Cloud-Labels {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
        virtual-private-cloud {
          network-label-key = "cromwell-ci-gcpbatch-network"
          # For GCP Batch we do not configure the subnetwork name. Batch has to work out the subnetwork for itself in
          # order to enable running jobs in regions that are different from the region of the GCP Batch to which we send jobs.
          auth = "service_account"
        }

        # Have the engine authenticate to docker.io. See BT-141 for more info.
        include "dockerhub_provider_config_v2.inc.conf"
      }
    }
    GCPBATCH-Virtual-Private-Cloud-Literals {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
        virtual-private-cloud {
          # integration testing:
          #  - fully qualified name
          #  - hardcoded project id
          #  - does not end with `/`
          network-name = "projects/broad-dsde-cromwell-dev/global/networks/cromwell-ci-gcpbatch-vpc-network"
          # For GCP Batch we do not reference the subnetwork name, Batch has to work that out for itself in order to
          # enable running jobs in regions that are different from the region of the GCP Batch to which we send jobs.
        }

        # Have the engine authenticate to docker.io. See BT-141 for more info.
        include "dockerhub_provider_config_v2.inc.conf"
      }
    }
    GCPBATCH-gcsa {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        project = "user_error: google_project must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        root = "user_error: jes_gcs_root must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        genomics.compute-service-account = "user_error: google_compute_service_account must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        genomics.auth = "google_compute_service_account"
        filesystems.http {}
        filesystems.drs.auth = "user_service_account"
        filesystems.gcs.auth = "user_service_account"
        filesystems.gcs.project = "user_error: user_service_account must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"

        # Have the engine authenticate to docker.io. See BT-141 for more info.
        include "dockerhub_provider_config_v2.inc.conf"
      }
    }
    GCPBATCHUSADockerhub {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        include "dockerhub_provider_config_v2_usa.inc.conf"
        # This SA does not have permission to bill this project when accessing RP buckets.
        # This is on purpose so that we can assert the failure (see requester_pays_localization_negative)
        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
      }
    }
  }
}
