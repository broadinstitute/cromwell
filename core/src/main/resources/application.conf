webservice {
  port = 8000
  interface = 0.0.0.0
  instance.name = "reference"
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  actor {
    default-dispatcher {
      fork-join-executor {
        # Number of threads = min(parallelism-factor * cpus, parallelism-max)
        # Below are the default values set by Akka, uncomment to tune these

        #parallelism-factor = 3.0
        #parallelism-max = 64
      }
    }

    deployment {
      /WorkflowManagerActor/WorkflowLogCopyRouter {
        router = round-robin-pool
        nr-of-instances = 10
      }
    }
  }

  dispatchers {
    # A dispatcher for actors performing blocking io operations
    # Prevents the whole system from being slowed down when waiting for responses from external resources for instance
    io-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      # Using the forkjoin defaults, this can be tuned if we wish
    }

    # A dispatcher for actors handling API operations
    # Keeps the API responsive regardless of the load of workflows being run
    api-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher for engine actors
    # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs
    # on its own dispatcher to prevent backends from affecting its performance.
    engine-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher used by supported backend actors
    backend-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # Note that without further configuration, all other actors run on the default dispatcher
  }
}

spray.can {
  server {
    request-timeout = 40s
  }
  client {
    request-timeout = 40s
    connecting-timeout = 40s
  }
}

system {
  // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  abort-jobs-on-terminate = false

  // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend
  max-retries = 10

  // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  workflow-restart = true

  // Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 5000

  // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 50

  // Number of seconds between workflow launches
  new-workflow-poll-rate = 20
}

workflow-options {
  // These workflow options will be encrypted when stored in the database
  encrypted-fields: []

  // AES-256 key to use to encrypt the values in `encrypted-fields`
  base64-encryption-key: "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="

  // Directory where to write per workflow logs
  workflow-log-dir: "cromwell-workflow-logs"

  // When true, per workflow logs will be deleted after copying
  workflow-log-temporary: true

  // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.
  // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:
  //workflow-failure-mode: "ContinueWhilePossible"
}

// Optional call-caching configuration. Disabled by default.
call-caching {
  enabled = false

  // The Docker image specified in the 'runtime' section of a task can be used as-is
  // or Cromwell can lookup this Docker image to get a complete hash.  For example,
  // if a task specifies docker: "ubuntu:latest" and if lookup-docker-hash is true,
  // Then Cromwell would query DockerHub to resolve "ubuntu:latest" to something like
  // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47
  //
  // A value of 'true' means that call hashes will more accurately represent the
  // Docker image that was used to run the call, but at a cost of having to make a
  // request to an external service (DockerHub, GCR).  If a call fails to lookup a
  // Docker hash, it will fail.
  lookup-docker-hash = false
}

google {

  application-name = "cromwell"

  auths = [
    {
      name = "application-default"
      scheme = "application_default"
    },
    {
      name = "user-via-refresh"
      scheme = "refresh_token"
      client-id = "secret_id"
      client-secret = "secret_secret"
    },
    {
      name = "service-account"
      scheme = "service_account"
      service-account-id = "my-service-account"
      pem-file = "/path/to/file.pem"
    }
  ]
}

engine {
  // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.
  // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.
  // If you intend to be able to run workflows with this kind of declarations:
  // workflow {
  //    String str = read_string("gs://bucket/my-file.txt")
  // }
  // You will need to provide the engine with a gcs filesystem
  // Note that the default filesystem (local) is always available.
//  filesystems {
//    gcs {
//      auth = "application-default"
//    }
//  }
}

backend {
  default = "Local"
  providers {
    Local {
      actor-factory = "cromwell.backend.impl.local.LocalBackendLifecycleActorFactory"
      config {
        // Root directory where Cromwell writes job results.  This directory must be
        // visible and writeable by the Cromwell process as well as the jobs that Cromwell
        // launches.
        root: "cromwell-executions"

        filesystems {
          local {
            // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs
            // The following are strategies used to make those links.  They are ordered.  If one fails
            // The next one is tried:
            //
            // hard-link: attempt to create a hard-link to the file
            // copy: copy the file
            // soft-link: create a symbolic link to the file
            //
            // NOTE: soft-link will be skipped for Docker jobs
            localization: [
              "hard-link", "soft-link", "copy"
            ]
          }
        }
      }
    }//,
    //    HtCondor {
    //      actor-factory = "cromwell.backend.impl.htcondor.HtCondorBackendFactory"
    //      config {
    //        // Root directory where Cromwell writes job results.  This directory must be
    //        // visible and writeable by the Cromwell process as well as the jobs that Cromwell
    //        // launches.
    //        root: "cromwell-executions"
    //
    //        cache {
    //          provider = "cromwell.backend.impl.htcondor.caching.provider.mongodb.MongoCacheActorFactory"
    //          enabled = true
    //          forceRewrite = false
    //          db {
    //            host = "127.0.0.1"
    //            port = 27017
    //            name = "htcondor"
    //            collection = "cache"
    //          }
    //        }
    //
    //        filesystems {
    //          local {
    //            // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs
    //            // The following are strategies used to make those links.  They are ordered.  If one fails
    //            // The next one is tried:
    //            //
    //            // hard-link: attempt to create a hard-link to the file
    //            // copy: copy the file
    //            // soft-link: create a symbolic link to the file
    //            //
    //            // NOTE: soft-link will be skipped for Docker jobs
    //            localization: [
    //              "hard-link", "soft-link", "copy"
    //            ]
    //          }
    //        }
    //      }
    //    },
    //    SGE {
    //      actor-factory = "cromwell.backend.impl.sge.SgeBackendLifecycleActorFactory"
    //      config {
    //        // Root directory where Cromwell writes job results.  This directory must be
    //        // visible and writeable by the Cromwell process as well as the jobs that Cromwell
    //        // launches.
    //        root: "cromwell-executions"
    //
    //        // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs
    //        // The following are strategies used to make those links.  They are ordered.  If one fails
    //        // The next one is tried:
    //        //
    //        // hard-link: attempt to create a hard-link to the file
    //        // copy: copy the file
    //        // soft-link: create a symbolic link to the file
    //        //
    //        // NOTE: soft-link will be skipped for Docker jobs
    //        filesystems = {
    //          local {
    //            localization: [
    //              "hard-link", "soft-link", "copy"
    //            ]
    //          }
    //        }
    //      }
    //    },
    //    JES {
    //      actor-factory = "cromwell.backend.impl.jes.JesBackendLifecycleFactory"
    //      config {
    //        // Google project
    //        project = "my-cromwell-workflows"
    //
    //        // Base bucket for workflow executions
    //        root = "gs://my-cromwell-workflows-bucket"
    //
    //        // Polling for completion backs-off gradually for slower-running jobs.
    //        // This is the maximum polling interval (in seconds):
    //        maximum-polling-interval = 600
    //
    //        // Optional Dockerhub Credentials. Can be used to access private docker images.
    //        dockerhub {
    //          // account = ""
    //          // token = ""
    //        }
    //
    //        genomics {
    //          // A reference to an auth defined in the `google` stanza at the top.  This auth is used to create
    //          // Pipelines and manipulate auth JSONs.
    //          auth = "application-default"
    //          // Endpoint for APIs, no reason to change this unless directed by Google.
    //          endpoint-url = "https://genomics.googleapis.com/"
    //        }
    //
    //        filesystems {
    //          gcs {
    //            // A reference to a potentially different auth for manipulating files via engine functions.
    //            auth = "user-via-refresh"
    //          }
    //        }
    //      }
    //    }
    }
  }

  services {
    KeyValue {
      class = "cromwell.services.KeyValueServiceActor"
    },
    MetadataService {
      class = "cromwell.services.metadata.EngineMetadataServiceActor"
    }
  }

  database {
    config = main.hsqldb

    main {
      hsqldb {
        db.url = "jdbc:hsqldb:mem:${slick.uniqueSchema};shutdown=false;hsqldb.tx=mvcc"
        db.driver = "org.hsqldb.jdbcDriver"
        db.connectionTimeout = 1000 // NOTE: 1000ms (the default) is ok for a small hsqldb, but often too short for mysql
        driver = "slick.driver.HsqldbDriver$"
      }
    }

    test {
      mysql {
        db.url = "jdbc:mysql://localhost/cromwell_test"
        db.user = "travis"
        db.password = ""
        db.driver = "com.mysql.jdbc.Driver"
        db.connectionTimeout = 5000 // NOTE: The default 1000ms is often too short for production mysql use
        driver = "slick.driver.MySQLDriver$"
      }
    }
  }
