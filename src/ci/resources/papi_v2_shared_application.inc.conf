include required(classpath("application.conf"))
include "build_application.inc.conf"
include "papi_application.inc.conf"

services {
  MetadataService {
    class = "cromwell.services.metadata.hybridcarbonite.HybridMetadataServiceActor"
    config {
      # This section may also contain the same set of options as would be present in the 'config' section of the
      # default (cromwell.services.metadata.impl.MetadataServiceActor) config

      # The carbonite section contains carbonite-specific options
      carbonite-metadata-service {
        # Which bucket to use for storing the generated metadata JSON
        bucket = "carbonite-test-bucket"

        # A filesystem able to access the specified bucket:
        filesystems {
          gcs {
            # A reference to the auth to use for storing and retrieving metadata:
            auth = "service_account"
          }
        }

        # The current Centaur Carbonite logic times out at 30 seconds waiting for terminal workflows to be Carbonited.
        # Set the backoff max interval for Carbonite freeze scanning below that limit.
        metadata-freezing {
          initial-interval = 2 seconds
          max-interval = 15 seconds
          multiplier = 1.1
          # Only carbonite workflows whose summary entry IDs are greater than or equal to this value:
          minimum-summary-entry-id = 0

          # To help us stay below the Travis "max log size" value:
          debug-logging = false
        }

        # Metadata deletion configuration.
        metadata-deletion {

          # How long to wait after system startup before the first metadata deletion action.
          # This is potentially useful to avoid overwhelming a nascent Cromwell restart with lots of deletion activity.
          initial-delay = 10 seconds

          # How often Cromwell should check for metadata ready for deletion. Set this value to "Inf" to turn off metadata deletion.
          # The default value is currently "Inf".
          interval = 15 seconds

          # Upper limit for the number of workflows which Cromwell will process during a single scheduled metadata deletion event.
          # The default value is currently "200".
          batch-size = 200

          # Minimum time between a workflow completion and deletion of its metadata from the database.
          # Note: Metadata is only eligible for deletion if it has already been carbonited.
          # The default value is currently "24 hours".
          delay-after-workflow-completion = 1 minute
        }
      }
    }
  }

  HealthMonitor.config {
    check-carboniter-gcs-access: true
    check-papi-backends: [
      "Papi",
      "Papiv2",
      "Papiv2USADockerhub",
      "Papiv2NoDockerHubConfig",
      "Papiv2RequesterPays",
      "Papi-Caching-No-Copy",
      "Papiv2-Virtual-Private-Cloud",
      "Papiv2-Retry-With-More-Memory",
    ]

    gcs-object-to-check-carboniter-access = "healthcheck.txt"
  }
}

backend {
  default = "Papi"
  enabled = ["Papi", "Papiv2", "Papi-Caching-No-Copy", "Papiv2RequesterPays"]
  providers {
    # Default papi v2 backend
    Papi {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        include "dockerhub_provider_config_v2.inc.conf"
        # This SA does not have permission to bill this project when accessing RP buckets.
        # This is on purpose so that we can assert the failure (see requester_pays_localization_negative)
        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
      }
    }
    # Same as Papi, but with a v2 specific name so it can be targeted in centaur tests
    Papiv2 {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        include "dockerhub_provider_config_v2.inc.conf"
        # This SA does not have permission to bill this project when accessing RP buckets.
        # This is on purpose so that we can assert the failure (see requester_pays_localization_negative)
        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
        name-for-call-caching-purposes = "Papi"
      }
    }
    # Same as Papi but specifying `user_service_account` auth in config.
    Papiv2USADockerhub {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        include "dockerhub_provider_config_v2_usa.inc.conf"
        # This SA does not have permission to bill this project when accessing RP buckets.
        # This is on purpose so that we can assert the failure (see requester_pays_localization_negative)
        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
        name-for-call-caching-purposes = "Papi"
      }
    }
    # Same as Papiv2 but with no Docker Hub configuration so access to private Docker Hub images will
    # require correct handling of workflow options.
    Papiv2NoDockerHubConfig {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        # This SA does not have permission to bill this project when accessing RP buckets.
        # This is on purpose so that we can assert the failure (see requester_pays_localization_negative)
        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
      }
    }
    Papiv2RequesterPays {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        include "dockerhub_provider_config_v2.inc.conf"
        filesystems.gcs.auth = "requester_pays_service_account"
        genomics.compute-service-account = "requester-pays-authorized@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
      }
    }
    Papi-Caching-No-Copy {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        filesystems.gcs.caching.duplication-strategy = "reference"
        filesystems.http {}
      }
    }
    Papiv2-Virtual-Private-Cloud {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
        virtual-private-cloud {
          network-label-key = "cromwell-ci-network"
          subnetwork-label-key = "cromwell-ci-subnetwork"
          auth = "service_account"
        }
      }
    }
    papi-v2-gcsa {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        project = "user_error: google_project must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        root = "user_error: jes_gcs_root must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        genomics.compute-service-account = "user_error: google_compute_service_account must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        genomics.auth = "google_compute_service_account"
        filesystems.http {}
        filesystems.drs.auth = "user_service_account"
        filesystems.gcs.auth = "user_service_account"
        filesystems.gcs.project = "user_error: user_service_account must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
      }
    }
    papi-v2-usa {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        filesystems.http {}
        filesystems.gcs.auth = "user_service_account"
        filesystems.drs.auth = "user_service_account"
        project = "user_error: google_project must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
        root = "user_error: jes_gcs_root must be set in workflow options http://cromwell.readthedocs.io/en/develop/wf_options/Google/"
      }
    }
    Papiv2-Retry-With-More-Memory {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        filesystems.http {}
        memory-retry {
          error-keys = ["OutOfMemoryError"]
          multiplier = 1.1
        }
      }
    }
    # Same as Papi but with GCS parallel composite uploads turned on.
    Papiv2ParallelCompositeUploads {
      actor-factory = "REPLACEME!"
      config {
        # When importing: Remember to also include an appropriate provider_config.inc.conf here.

        include "dockerhub_provider_config_v2.inc.conf"
        # This SA does not have permission to bill this project when accessing RP buckets.
        # This is on purpose so that we can assert the failure (see requester_pays_localization_negative)
        genomics.compute-service-account = "centaur@broad-dsde-cromwell-dev.iam.gserviceaccount.com"
        # Files larger than 150M should be delocalized using parallel composite uploading.
        genomics.parallel-composite-upload-threshold = 150M
        filesystems.http {}
        name-for-call-caching-purposes = "Papi"
      }
    }
  }
}
